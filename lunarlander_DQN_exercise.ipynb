{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trains an agent with DQN on LunarLander. Uses OpenAI Gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### hyperparameters and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters and configurations\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99  # discount factor for reward\n",
    "resume = True  # resume from previous checkpoint?\n",
    "render = True  # render the graphic ?\n",
    "is_train = False # training mode ?\n",
    "max_episode_number = 1000  # how many episode we want to run ?\n",
    "max_replay_memory = 2000 # maximum size of replay memory\n",
    "epsilon_decay_steps = 500 # epsilon decay over time until ?\n",
    "batch_size = 32 # training examples per batch\n",
    "train_freq = 4 # train the model every \"train_freq\" steps\n",
    "model_path = \"_models/qlearning/model.ckpt\"  # path for saving the model\n",
    "\n",
    "\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate, scope=\"policy_network\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            # Define input placeholder\n",
    "            self.state = ??? # shape = [None, state_space_size]\n",
    "            self.action = ??? # shape = [None]\n",
    "            self.reward = ??? # shape = [None]\n",
    "\n",
    "            # Fully connected layer with 64 units and activation function relu\n",
    "            # Hint: tf.layers.dense\n",
    "            fc1 = ???\n",
    "\n",
    "            # Fully connected layer with 64 units and activation function relu\n",
    "            # Hint: tf.layers.dense\n",
    "            fc2 = ???\n",
    "\n",
    "            # last linear layer producing predicted q value for all actions\n",
    "            self.prediction = ???\n",
    "            \n",
    "            # Get the predictions for the chosen actions only\n",
    "            # Hint: use onehot_action with prediction\n",
    "            onehot_action = tf.one_hot(self.action, 4)\n",
    "            self.action_predictions = ???\n",
    "            \n",
    "            # Calcualte the loss\n",
    "            # loss = mean((target_value - action_prediction)^2)\n",
    "            self.loss = ???\n",
    "\n",
    "            # train op\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "    def predict(self, state, sess):\n",
    "        return sess.run(self.action_prob, {self.state: state})\n",
    "\n",
    "    def update(self, state, reward, action, num_step, sess):\n",
    "        feed_dict = {self.state: state, self.reward: reward, self.action: action}\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define epsilon_greedy policy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_values, epsilon, action_space_size):\n",
    "    \"\"\"\n",
    "    choose action randomly with epsilon probability\n",
    "    otherwise choose action with maximum value\n",
    "    \n",
    "    :param q_values: estimated q values for all actions\n",
    "    :param epsilon: exploration probability\n",
    "    :param action_space_size: size of action space\n",
    "    :return: a chosen action\n",
    "    \"\"\"\n",
    "    \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize network and create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "replay_memory = []\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "num_steps = 0\n",
    "loss = None\n",
    "\n",
    "q_estimator = ValueNetwork(learning_rate, scope=\"q_estimator\")\n",
    "# The epsilon decay schedule\n",
    "epsilons = np.linspace(1.0, 0.1, 1000)\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver()\n",
    "# session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if resume:\n",
    "    saver.restore(sess, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start the first episode\n",
    "observation = env.reset()\n",
    "while episode_number < max_episode_number:\n",
    "    if render: env.render()\n",
    "\n",
    "    current_state = observation\n",
    "\n",
    "    # forward the value network and get predicted q values\n",
    "    q_values = q_estimator.predict(current_state[np.newaxis, :], sess)\n",
    "\n",
    "    if is_train:\n",
    "        epsilon = epsilons[min(episode_number, epsilon_decay_steps-1)]\n",
    "    else:\n",
    "        epsilon = 0.05\n",
    "        \n",
    "    action = epsilon_greedy(q_values.ravel(), epsilon, 4)\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "\n",
    "    # record transition into replay memory\n",
    "    if len(replay_memory) > max_replay_memory:\n",
    "        replay_memory.pop(0)\n",
    "    replay_memory.append(Transition(current_state, action, reward, observation, done))\n",
    "\n",
    "    num_steps += 1\n",
    "\n",
    "    if num_steps % train_freq == 0:\n",
    "        # Sample a minibatch of batch_size from the replay memory\n",
    "        samples = ???\n",
    "        states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "        # Calculate q values and targets\n",
    "        q_values_next = ??? # next_state's q_values\n",
    "        targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * gamma * np.amax(\n",
    "            q_values_next, axis=1)\n",
    "\n",
    "        # Perform gradient descent update\n",
    "        loss = q_estimator.update(states_batch, targets_batch, action_batch, sess)\n",
    "\n",
    "    if done:\n",
    "        print(\"loss: \" + str(loss))\n",
    "        print(\"epsilon \" + str(epsilon))\n",
    "        episode_number += 1\n",
    "\n",
    "        observation = env.reset()  # reset env\n",
    "\n",
    "        # record running_reward to get overview of the improvement so far\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "\n",
    "        print('ep %d: game finished, reward: %.2f, running_reward: %.2f' % (\n",
    "            episode_number, reward_sum, running_reward))\n",
    "\n",
    "        # reset reward_sum\n",
    "        reward_sum = 0\n",
    "\n",
    "        # save the model every 20 episodes\n",
    "        if episode_number % 20 == 0: saver.save(sess, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
