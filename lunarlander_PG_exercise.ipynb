{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trains an agent with (stochastic) Policy Gradients on LunarLander. Uses OpenAI Gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "gamma = 0.99  # discount factor for reward\n",
    "resume = True  # resume from previous checkpoint?\n",
    "render = True  # render the graphic ?\n",
    "max_episode_number = 1000 # how many episode we want to run ?\n",
    "model_path = \"_models/policygradient/model.ckpt\" # path for saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate, scope=\"policy_network\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            # Define input placeholder\n",
    "            self.state = ??? # shape = [None, state_space_size]\n",
    "            self.action = ??? # shape = [None]\n",
    "            self.reward = ??? # shape = [None]\n",
    "\n",
    "            # Fully connected layer with 16 units and activation function tanh\n",
    "            # Hint: tf.layers.dense\n",
    "            fc1 = ???\n",
    "\n",
    "            # Fully connected layer with 32 units and activation function tanh\n",
    "            # Hint: tf.layers.dense\n",
    "            fc2 = ???\n",
    "\n",
    "            # logits layer before applying softmax with number of units = number of possible actions\n",
    "            logits = ???\n",
    "\n",
    "            # action prob = softmax(logits)\n",
    "            self.action_prob = ???\n",
    "            \n",
    "            # calculate -log(prob)\n",
    "            # Hint: tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "            neg_log_prob = ???\n",
    "\n",
    "            # calculate loss: mean(-log(prob) * reward)\n",
    "            self.loss = ???\n",
    "            \n",
    "            # train op\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "    def predict(self, state, sess):\n",
    "        return sess.run(self.action_prob, {self.state: state})\n",
    "\n",
    "    def update(self, state, reward, action, num_step, sess):\n",
    "        feed_dict = {self.state: state, self.reward: reward, self.action: action}\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for calculating discounted rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\"\n",
    "    take 1D float array of rewards and compute discounted rewards (A_t)\n",
    "    A_t = R_t + gamma^1 * R_t+1 + gamma^2 * R_t+2 + ... + gamma^(T-t)R_T;\n",
    "    where T is the last time step of the episode\n",
    "    \n",
    "    :param r: float array of rewards (R_1, R_2, ..., R_T)\n",
    "    :return: float array of discounted reward (A_1, A_2, ..., A_T)\n",
    "    \"\"\"\n",
    "    \n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize network and create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "state_list, action_list, reward_list = [], [], []\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "num_step = 0\n",
    "\n",
    "policy_network = PolicyNetwork(learning_rate)\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver()\n",
    "# session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if resume:\n",
    "    saver.restore(sess, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start the first episode\n",
    "observation = env.reset()\n",
    "while episode_number < max_episode_number:\n",
    "    if render: env.render()\n",
    "\n",
    "    current_state = observation\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    action_prob = policy_network.predict(current_state[np.newaxis, :], sess)\n",
    "    action = np.random.choice(a=4, p=action_prob.ravel())\n",
    "\n",
    "    # record various intermediates\n",
    "    action_list.append(action) # actions\n",
    "    state_list.append(current_state)  # states\n",
    "    \n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    reward_sum += reward\n",
    "    reward_list.append(reward)  # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done:  # an episode finished\n",
    "        episode_number += 1\n",
    "\n",
    "        # stack together all inputs, action and rewards for this episode\n",
    "        state_batch = np.vstack(state_list)\n",
    "        action_batch = np.array(action_list)\n",
    "        reward_batch = np.array(reward_list)\n",
    "\n",
    "        state_list, action_list, reward_list = [], [], []  # reset array memory\n",
    "\n",
    "        # compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(reward_batch)\n",
    "        # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "        # update model variables with data obtained from this episode\n",
    "        policy_network.update(state_batch, discounted_epr, action_batch, episode_number, sess)\n",
    "\n",
    "        # record running_reward to get overview of the improvement so far\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "\n",
    "        observation = env.reset()  # reset env\n",
    "\n",
    "        print('ep %d: game finished, reward: %.2f, running_reward: %.2f' % (\n",
    "            episode_number, reward_sum, running_reward))\n",
    "\n",
    "        # reset reward_sum\n",
    "        reward_sum = 0\n",
    "\n",
    "        # save the model every 30 episodes\n",
    "        if episode_number % 30 == 0: saver.save(sess, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
